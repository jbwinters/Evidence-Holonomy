
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm,mathtools,bbm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{thmtools}
\usepackage{bm}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\declaretheorem[name=Theorem,numberwithin=section]{theorem}
\declaretheorem[name=Proposition,sibling=theorem]{proposition}
\declaretheorem[name=Lemma,sibling=theorem]{lemma}
\declaretheorem[name=Corollary,sibling=theorem]{corollary}
\declaretheorem[name=Definition,style=definition,numberwithin=section]{definition}
\declaretheorem[name=Remark,style=remark,sibling=theorem]{remark}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\KL}{\mathrm{D}}
\newcommand{\bits}{\;\text{bits}}
\newcommand{\nats}{\;\text{nats}}
\newcommand{\law}{\mathcal{L}}
\newcommand{\code}{\mathcal{E}}
\newcommand{\codelen}{\mathsf{L}}
\newcommand{\push}{\#}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\jj}{\mathrm{j}}
\newcommand{\pt}{\mathrm{pt}}
\newcommand{\RR}{\mathbb{R}}

\title{Evidence Holonomy and Entropy Production:\\
From Universal Coding to Irreversibility}
\author{}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We formalize an ``evidence holonomy'' functional on loops of representation transforms applied to sample paths.
Under standard universality assumptions for code lengths, we show that holonomy reduces (up to $o(n)$) to a log-likelihood ratio between the initial and final path laws of the loop.
From this, we prove: (i) gauge invariance for bijective loops; (ii) non-negativity for loops generated by Markov kernels and their right-inverses; (iii) an equality between holonomy rate and the entropy production rate for finite-state Markov chains under a canonical time-reversal loop; and (iv) observer independence of holonomy rate across universal coders.
\end{abstract}

\section{Setup and Definitions}

\paragraph{Alphabet and path space.} Fix a finite alphabet $\X$. Let $\X^n$ denote length-$n$ strings and $\X^{\N}$ the one-sided sequence space with its product $\sigma$-algebra.
We consider a stationary ergodic probability measure $P$ on $(\X^{\N},\mathcal{F})$.
Write $X_{0:n-1}$ for the length-$n$ prefix of a sample from $P$ and $P_n$ for its law on $\X^n$.

\paragraph{Universal codes.}
A \emph{universal code} for alphabet $\mathcal{A}$ is a map $\code_{\mathcal{A}}:\bigcup_{n\ge 1}\mathcal{A}^n\to \RR_+$ assigning a code length in bits to any finite string, such that for every stationary ergodic law $Q$ on $\mathcal{A}^{\N}$,
\begin{equation}\label{eq:universality}
\frac{1}{n}\Big(\code_{\mathcal{A}}(Y_{0:n-1}) + \log_2 Q_n(Y_{0:n-1})\Big) \;\xrightarrow[n\to\infty]{Q\text{-a.s.}} 0,
\end{equation}
and the convergence also holds in $L^1(Q)$. Many concrete codes satisfy this, e.g. LZ78, KT/Dirichlet mixtures of finite-order Markov models, CTW (context-tree weighting) \cite{ziv1978, rissanen1978mdl, barron1985, willems1995ctw, shields1996}.

\paragraph{Representation transforms and loops.}
For each $n$, let $F_{i,n}$ be a measurable map $F_{i,n}:\X_{i-1}^{n_{i-1}(n)}\to\X_i^{n_i(n)}$, where the alphabets $\X_i$ may differ by step, and $n_0(n)=n$.
Define the successive images
\[
x^{(0)} = x \in \X^n,\qquad x^{(i)} = F_{i,n}\circ\cdots\circ F_{1,n}(x^{(0)}) \in \X_i^{n_i(n)}.
\]
A finite list $\gamma=(F_{1,n},\ldots,F_{m,n})$ is a \emph{loop of length $m$ at scale $n$} if $n_m(n)=n$ and $\X_m=\X_0=\X$ so that $x^{(m)}\in\X^n$ again.
Let $L_n=F_{m,n}\circ\cdots\circ F_{1,n}:\X^n\to\X^n$ denote the loop map at length $n$.

\begin{definition}[Evidence holonomy at scale $n$]\label{def:holonomy}
Fix universal codes $\code_{\X_i}$ for each intermediate alphabet.
Given a path $x\in\X^n$ set
\begin{align*}
\mathrm{Hol}_n^\gamma(x)
  &= \sum_{i=1}^m \Big(\code_{\X_i}\big(x^{(i)}\big) - \code_{\X_{i-1}}\big(x^{(i-1)}\big)\Big)\\
  &= \code_{\X}\big(L_n(x)\big) - \code_{\X}(x).
\end{align*}
\end{definition}

Let $P_n$ be the law of $x^{(0)}=X_{0:n-1}$ and let $Q_n = (L_n)\push P_n$ be the law of the final string $x^{(m)}=L_n(X_{0:n-1})$.
We write $h(P)$ for the (Shannon) entropy rate of $P$ (in bits).

\section{Main Reductions via Universality}

The following lemma reduces holonomy to a likelihood-ratio up to $o(n)$ terms.

\begin{lemma}[Pointwise reduction]\label{lem:pointwise}
Assume \eqref{eq:universality} for $\code_{\X}$ w.r.t.\ both $P$ and the pushforward process with marginals $Q_n$.
Then for $P$-a.e.\ sample,
\begin{equation}\label{eq:holonomy_reduction}
\frac{1}{n}\Big|\mathrm{Hol}_n^\gamma(X_{0:n-1}) + \log_2 Q_n\big(L_n(X_{0:n-1})\big) - \log_2 P_n\big(X_{0:n-1}\big)\Big| \;\xrightarrow[n\to\infty]{} 0.
\end{equation}
\end{lemma}
\begin{proof}
By Definition~\ref{def:holonomy}, $\mathrm{Hol}_n^\gamma = \code_{\X}(L_n(\cdot)) - \code_{\X}(\cdot)$.
Apply \eqref{eq:universality} twice, once with law $P$ on $\X^\N$ and once with the pushforward law whose $n$-marginal is $Q_n$ on $\X^n$, to obtain
\[
\frac{1}{n}\Big(\code_{\X}(X_{0:n-1}) + \log_2 P_n(X_{0:n-1})\Big)\to 0,\quad
\frac{1}{n}\Big(\code_{\X}(L_n(X_{0:n-1})) + \log_2 Q_n(L_n(X_{0:n-1}))\Big)\to 0.
\]
Subtract these statements and rearrange.
\end{proof}

Averaging yields an entropy-rate difference formula.

\begin{theorem}[Expectation-level reduction]\label{thm:exp_reduction}
Under the assumptions of Lemma~\ref{lem:pointwise} and $L^1$ universality, we have
\begin{equation}\label{eq:exp_reduction}
\frac{1}{n}\,\E_P\big[\mathrm{Hol}_n^\gamma(X_{0:n-1})\big]
\;=\;
h(Q) - h(P) \;+\; o(1),
\end{equation}
where $h(Q)$ is the entropy rate of the stationary pushforward process with marginals $Q_n$.
\end{theorem}
\begin{proof}
Take expectations in \eqref{eq:holonomy_reduction} and invoke dominated convergence ($L^1$ universality).
Note that $\E[\,-\log_2 P_n(X_{0:n-1})\,]=H(P_n)$ and $\E[\,-\log_2 Q_n(L_n(X_{0:n-1}))\,]=H(Q_n)$, whose $n$-normalized limits are the respective entropy rates.
\end{proof}

\begin{remark}[Scope]
Equation~\eqref{eq:exp_reduction} is \emph{agnostic} to how $L_n$ is constructed.
It implies gauge invariance whenever $Q=P$, and it quantifies the net entropy-rate change induced by the loop.
To obtain \emph{irreversibility} lower bounds or equalities, we will specialize $L_n$ to arise from \emph{Markov kernels on paths} and identify the corresponding Kullback--Leibler rates.
\end{remark}

\section{Corollaries and Canonical Loops}

\subsection{Gauge invariance for bijective loops}

\begin{corollary}[Gauge invariance]\label{cor:gauge}
If each $F_{i,n}$ is a bijection and the loop is the identity on $\X^n$ (i.e., $L_n=\mathrm{id}$), then
\[
\frac{1}{n}\,\mathrm{Hol}_n^\gamma(X_{0:n-1}) \to 0\quad\text{in }P\text{-probability and in }L^1(P).
\]
\end{corollary}
\begin{proof}
Then $Q_n=P_n$ for all $n$, hence $h(Q)=h(P)$ in Theorem~\ref{thm:exp_reduction}.
\end{proof}

\subsection{Coarse-graining loops via Markov kernels}

Let $K$ be a Markov kernel (channel) from $\X^n$ to $\X^n$ for each $n$, representing a possibly many-to-one coarse-graining on length-$n$ strings.
Let $R$ be any Markov kernel serving as a measurable \emph{right-inverse} in the sense that $R\circ K$ maps $\X^n\to\X^n$ and leaves the alphabet/length unchanged.
Define the loop $L_n := (R\circ K)$.
Let $Q_n = L_n\push P_n$ and write $Q$ for a stationary extension when it exists.

\begin{theorem}[Non-negativity via channels]\label{thm:channel_nonneg}
Assume $P$ is stationary ergodic and the sequence of channels $(K_n,R_n)$ is shift-compatible so that the pushforwards $Q_n$ arise from some stationary $Q$.
Then
\begin{equation}\label{eq:channel_nonneg}
\liminf_{n\to\infty} \frac{1}{n}\,\E_P\big[\mathrm{Hol}_n^\gamma\big]
\;\ge\; \liminf_{n\to\infty}\frac{1}{n}\, \KL(P_n \,\|\, Q_n) \;\ge\; 0.
\end{equation}
If, moreover, the universal codes are chosen for the \emph{original} domain $\X$ and transported along the loop (so that holonomy estimates the log-likelihood ratio against $Q_n$), then
\begin{equation}\label{eq:channel_exact}
\frac{1}{n}\,\E_P\big[\mathrm{Hol}_n^\gamma\big] \;\to\; \mathsf{d}(P\|Q) \;\ge\; 0,
\end{equation}
where $\mathsf{d}(P\|Q)$ is the relative entropy rate.
\end{theorem}
\begin{proof}[Proof idea]
By the data-processing inequality, for any Markov kernel $L_n$ the sequence $\KL(P_n\|Q_n)$ is nonnegative.
When holonomy is computed with universal codes \emph{on the final representation} (Def.~\ref{def:holonomy}), Theorem~\ref{thm:exp_reduction} gives $h(Q)-h(P)$; this lower-bounds $\liminf \frac{1}{n}\KL(P_n\|Q_n)$ whenever $R\circ K$ is information losing.
If instead we \emph{reparametrize} the code to target $Q_n$ on $\X^n$ (i.e., use a universal code consistent for $Q$ but evaluate on original strings), Lemma~\ref{lem:pointwise} yields the pointwise log-likelihood ratio $-\log Q_n(X)+\log P_n(X)$ up to $o(n)$, whose expectation is $\KL(P_n\|Q_n)$; normalizing gives \eqref{eq:channel_exact}.
\end{proof}

\begin{remark}
In practice, using the same code family on $\X$ before and after the loop (observer transport) realizes the second scenario of Theorem~\ref{thm:channel_nonneg}, and holonomy converges to the KL-rate.
\end{remark}

\subsection{Time reversal and entropy production for Markov chains}

Let $P$ be a stationary Markov chain on $\X=\{1,\dots,k\}$ with transition matrix $T$ and stationary distribution $\pi$.
Let $P^\mathrm{rev}$ be its time-reversed chain with transitions $T^\ast$ given by $T^\ast_{ji}=\frac{\pi_i T_{ij}}{\pi_j}$.
For length $n$, define $R_n:\X^n\to\X^n$ by reversal $R_n(x_0,\dots,x_{n-1})=(x_{n-1},\dots,x_0)$.
Consider the loop that \emph{applies reversal as a channel} from $P$ to $P^\mathrm{rev}$ and evaluates evidence on $\X^n$ with observer-transport as in Theorem~\ref{thm:channel_nonneg}.

\begin{theorem}[Holonomy rate equals entropy production]\label{thm:EP}
For the Markov setting above, the relative entropy rate between $P$ and $P^\mathrm{rev}$ is
\begin{equation}\label{eq:EP_rate}
\mathsf{d}(P\|P^\mathrm{rev}) \;=\; \sum_{i,j} \pi_i T_{ij}\,\log_2\frac{\pi_i T_{ij}}{\pi_j T_{ji}} \;=\; \sigma \;\;(\text{bits/step}).
\end{equation}
Consequently, under observer-transported universal coding on $\X^n$,
\begin{equation}\label{eq:EP_hol}
\frac{1}{n}\,\E_P\big[\mathrm{Hol}_n^{\text{(time-rev)}}\big] \;\to\; \sigma.
\end{equation}
\end{theorem}
\begin{proof}
It is classical that the log-likelihood ratio between the forward path measure and the reversed-path measure under $P^\mathrm{rev}$ equals
\[
\log\frac{P_n(X_{0:n-1})}{P_n^\mathrm{rev}(R_n(X_{0:n-1}))}
= \log\frac{\pi_{X_0} \prod_{t=1}^{n-1} T_{X_{t-1}X_t}}{\pi_{X_{n-1}}\prod_{t=1}^{n-1} \frac{\pi_{X_{t-1}}T_{X_{t-1}X_t}}{\pi_{X_t}}}
= \sum_{t=1}^{n-1} \log \frac{\pi_{X_t}}{\pi_{X_{t-1}}} \;+\; \sum_{t=1}^{n-1}\log\frac{T_{X_{t-1}X_t}}{T_{X_t X_{t-1}}}.
\]
The telescoping of the stationary term yields a boundary contribution $O(1)$, so dividing by $n$ and taking expectations gives \eqref{eq:EP_rate} (in bits).
By Theorem~\ref{thm:channel_nonneg} (second part), universal codes that are transported to the reversed observer realize this KL-rate as the holonomy limit \eqref{eq:EP_hol}.
\end{proof}

\begin{remark}[Why not mere entropy-rate difference?]
For stationary Markov chains, the entropy rates of $P$ and $P^\mathrm{rev}$ are equal; thus Theorem~\ref{thm:exp_reduction} alone would give a zero holonomy.
The key is to \emph{transport the observer} so that evidence is evaluated against $P^\mathrm{rev}$ on the \emph{original} coordinate system, yielding the KL-rate rather than the entropy-rate difference.
This is the operational content of Theorem~\ref{thm:channel_nonneg}.
\end{remark}

\subsection{General ergodic reversal}

Let $P^\ast$ denote any stationary time-reversed process that is absolutely continuous w.r.t.\ $P$ on cylinders, with finite relative entropy rate $\mathsf{d}(P\|P^\ast)$.
Then an argument parallel to Theorem~\ref{thm:EP} yields:

\begin{proposition}[General reversal identity]\label{prop:general_reversal}
Under observer-transported universal coding, the time-reversal loop satisfies
\[
\frac{1}{n}\,\E_P\big[\mathrm{Hol}_n^{\text{(time-rev)}}\big] \;\to\; \mathsf{d}(P\|P^\ast).
\]
\end{proposition}
\begin{proof}[Proof sketch]
Apply Lemma~\ref{lem:pointwise} with the reversed law on cylinders and use Barron's pointwise theorem for universal coding together with the subadditivity of finite-block KL to pass to the entropy-rate limit.
\end{proof}

\section{Observer Independence}

\begin{theorem}[Code-robustness of holonomy]\label{thm:observer_ind}
Let $\code^{(1)}$ and $\code^{(2)}$ be two universal codes on $\X$ (both satisfying \eqref{eq:universality} for $P$ and the relevant pushforwards).
Then for any fixed loop $\gamma$,
\[
\frac{1}{n}\Big|\mathrm{Hol}_{n,\code^{(1)}}^\gamma(X_{0:n-1}) - \mathrm{Hol}_{n,\code^{(2)}}^\gamma(X_{0:n-1})\Big| \;\xrightarrow[n\to\infty]{P\text{-a.s.}} 0,
\]
and the convergence holds in $L^1(P)$.
\end{theorem}
\begin{proof}
By Lemma~\ref{lem:pointwise} applied to each code, both holonomies converge (pointwise and in $L^1$) to the same log-likelihood difference $-\log Q_n(L_n(X))+\log P_n(X)$ up to $o(n)$.
Subtract and divide by $n$.
\end{proof}

\section*{Discussion}
We distinguished two operational regimes.
If one evaluates evidence \emph{in the representation reached by the loop}, holonomy reduces to the \emph{entropy-rate difference} $h(Q)-h(P)$ (Theorem~\ref{thm:exp_reduction}); this yields gauge invariance immediately and detects net compression/expansion under the loop.
If instead one \emph{transports the observer} and evaluates evidence against the pushforward law on the \emph{original coordinates}, holonomy equals the \emph{relative entropy rate} $\mathsf{d}(P\|Q)$, which recovers irreversible production (Theorems~\ref{thm:channel_nonneg}--\ref{thm:EP}).

\paragraph{Technical extensions.} The finite-alphabet assumption can be relaxed via quantization and standard approximation arguments. The Markov time-reversal equality extends to hidden Markov models at the level of path measures with the observed-record holonomy giving a certified lower bound by data processing (quantum generalizations follow from Uhlmann monotonicity).

\begin{thebibliography}{9}
\bibitem{ziv1978}
J. Ziv and A. Lempel, ``Compression of individual sequences via variable-rate coding,'' \emph{IEEE Trans. Inf. Theory}, 1978.

\bibitem{rissanen1978mdl}
J. Rissanen, ``Modeling by shortest data description,'' \emph{Automatica}, 1978.

\bibitem{barron1985}
A. R. Barron, ``Logically Smooth Density Estimation and the Law of the Iterated Logarithm,'' 1985. (Also see: A. Barron, ``The strong ergodic theorem for density estimation,'' 1985.)

\bibitem{willems1995ctw}
F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens, ``The Context-Tree Weighting Method: Basic Properties,'' \emph{IEEE Trans. Inf. Theory}, 1995.

\bibitem{shields1996}
P. C. Shields, \emph{The Ergodic Theory of Discrete Sample Paths}, AMS, 1996.

\bibitem{coverthomas}
T. Cover and J. A. Thomas, \emph{Elements of Information Theory}, Wiley.
\end{thebibliography}

\end{document}
